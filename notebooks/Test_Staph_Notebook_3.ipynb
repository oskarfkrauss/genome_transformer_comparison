{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeac3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0 install required packages TODO: rename this notebook\n",
    "%pip install transformers torch pandas numpy matplotlib networkx seaborn scikit-learn umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d551483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oskar/miniconda3/envs/genome_transformer/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# step 1 import required packages\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0247ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# step 2: Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7d1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: function to parse FASTA files\n",
    "def parse_fasta(file_path: str):\n",
    "    '''\n",
    "    Parse fasta file (.fna) file into list of strings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the fasta sequence file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    seq : List\n",
    "        The sequence parsed so that each element is a line from the fasta file\n",
    "    '''\n",
    "    with open(file_path) as f:\n",
    "        seq = []\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            # do not parse the header line\n",
    "            if line.startswith('>'):\n",
    "                continue\n",
    "            else:\n",
    "                seq.append(line)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9425a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATGTCGGAAAAAGAAATTTGGGAAAAAGTGCTTGAAATTGCTCAAGAAAAATTATCAGCTGTAAGTTACTCAACTTTCCT\n"
     ]
    }
   ],
   "source": [
    "# step 4: parse sequences from data file, at the moment, NOTE: at the moment\n",
    "# we are only considering the first line of the sequence\n",
    "strain_1 = parse_fasta(os.path.join('data', 'GCA_006094915.1', 'GCA_006094915.1_ASM609491v1_genomic.fna'))[0]\n",
    "strain_2 = parse_fasta(os.path.join('data', 'GCA_026167765.1', 'GCA_026167765.1_ASM2616776v1_genomic.fna'))[0]\n",
    "strain_3 = parse_fasta(os.path.join('data', 'GCA_900607265.1', 'GCA_900607265.1_BPH2003_genomic.fna'))[0]\n",
    "strain_4 = parse_fasta(os.path.join('data', 'GCA_900620245.1', 'GCA_900620245.1_BPH2947_genomic.fna'))[0]\n",
    "\n",
    "print(strain_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cff98b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: Define masked embedding function\n",
    "def get_masked_embedding(sequence: str):\n",
    "    '''\n",
    "    Create an embedding of a genome sequence\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : str\n",
    "        The sequence we parsed earlier, as a single string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The mean embedding for the tokenized sequence\n",
    "    '''\n",
    "    # this splits the sequence into input:\n",
    "    #  - IDs e.g. [2, 312, ... , 3671] which each correspond to a different 6-mer string in the\n",
    "    # tokenizer's vocabulary\n",
    "    #  - an attention mask e.g. [1, 1, 1, 0, 0] which tells the embedding model which IDs to\n",
    "    # process. In our case (for now), it'll be tensor of 1s since we are including every 6-mer\n",
    "    # string\n",
    "    tokens = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    # input_ids = tokens[\"input_ids\"][0].tolist()\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    # print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    # print(input_ids)\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    # outputs is an encoding for each *token* for each layer of the (32-layer) Transformer,\n",
    "    # it has length 33 since layer 0 is an 'embedding layer'\n",
    "    # to get the 'final' embeddings, take the last layer NOTE: we may not always want the last\n",
    "    # layer\n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    print(embeddings.shape)\n",
    "    print(embeddings)\n",
    "    attention_mask = attention_mask.unsqueeze(-1)\n",
    "    masked_embeddings = embeddings * attention_mask\n",
    "    mean_embedding = masked_embeddings.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return mean_embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1db1c547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 2560])\n",
      "tensor([[[-0.0720,  0.2746,  0.0986,  ...,  0.0417, -0.0201,  0.2377],\n",
      "         [ 0.3900, -0.5494,  0.8788,  ..., -0.4445,  0.5566,  0.3560],\n",
      "         [ 0.4664, -0.9278,  0.3667,  ..., -0.2012,  0.7068, -0.1666],\n",
      "         ...,\n",
      "         [-0.7106,  0.3668,  0.8364,  ..., -0.6543, -0.5756,  0.1532],\n",
      "         [ 0.6815, -0.2832,  0.1981,  ...,  0.6719,  0.0036,  0.3221],\n",
      "         [-0.5512, -0.6719, -0.2787,  ...,  0.5013,  0.5288,  0.0133]]])\n",
      "2560\n"
     ]
    }
   ],
   "source": [
    "# step 5: generate mean embeddings for each strain\n",
    "embedding_for_strain_1 = get_masked_embedding(strain_1)\n",
    "print(len(embedding_for_strain_1))\n",
    "# embedding_for_strain_2 = get_masked_embedding(strain_2)\n",
    "# embedding_for_strain_3 = get_masked_embedding(strain_3)\n",
    "# embedding_for_strain_4 = get_masked_embedding(strain_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed78f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6: Create dataframe with embeddings\n",
    "headers = ['strain_1', 'strain_2', 'strain_3', 'strain_4']\n",
    "embeddings = [\n",
    "    embedding_for_strain_1,\n",
    "    embedding_for_strain_2,\n",
    "    embedding_for_strain_3,\n",
    "    embedding_for_strain_4\n",
    "]\n",
    "\n",
    "df_all = pd.DataFrame({\n",
    "    'Header': headers,\n",
    "    'embedding': embeddings\n",
    "})\n",
    "\n",
    "# step 7: Compute cosine similarity matrix\n",
    "X = np.stack(df_all['embedding'].values)\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# step 8: Visualize similarity matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarity_matrix, xticklabels=df_all['Header'], yticklabels=df_all['Header'],\n",
    "            annot=True, cmap='viridis')\n",
    "plt.title(\"Cosine Similarity Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# step 9: Clustering using DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=0.1, min_samples=2, metric='cosine').fit(X)\n",
    "df_all['Cluster'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 10: Build similarity graph\n",
    "import networkx as nx\n",
    "\n",
    "threshold = 0.85  # Adjust as needed\n",
    "G = nx.Graph()\n",
    "for i, label_i in enumerate(df_all['Header']):\n",
    "    G.add_node(label_i)\n",
    "    for j in range(i+1, len(df_all)):\n",
    "        if similarity_matrix[i][j] >= threshold:\n",
    "            G.add_edge(label_i, df_all['Header'][j], weight=similarity_matrix[i][j])\n",
    "\n",
    "# step 11: Visualise similarity graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw_networkx_nodes(G, pos, node_color='skyblue', node_size=800)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray')\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in G.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "plt.title(\"Genome Similarity Graph (Cosine ≥ 0.85)\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f1449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 12: PCA visualisation\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "jitter = np.random.normal(0, 0.01, X_pca.shape)\n",
    "X_pca_jittered = X_pca + jitter\n",
    "\n",
    "unique_clusters = sorted(df_all['Cluster'].unique())\n",
    "color_map = {cluster: plt.cm.tab10(i % 10) for i, cluster in enumerate(unique_clusters)}\n",
    "colors = [color_map[c] for c in df_all['Cluster']]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca_jittered[:,0], X_pca_jittered[:,1], c=colors)\n",
    "for i, label in enumerate(df_all['Header']):\n",
    "    plt.text(X_pca_jittered[i,0], X_pca_jittered[i,1], label, fontsize=8)\n",
    "plt.title(\"PCA of Genome Embeddings with Clustering\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04912da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 13: euclidean distance matrix and visualisation\n",
    "from sklearn.metrics import pairwise_distances\n",
    "distance_matrix = pairwise_distances(X, metric='euclidean')\n",
    "\n",
    "sns.heatmap(distance_matrix, xticklabels=df_all['Header'], yticklabels=df_all['Header'], annot=True, cmap='coolwarm')\n",
    "plt.title(\"Euclidean Distance Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 14: UMAP visualisation for non linear relationships between embeddings\n",
    "from umap import UMAP\n",
    "umap = UMAP(n_neighbors=5, min_dist=0.3, metric='cosine')\n",
    "X_umap = umap.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_umap[:,0], X_umap[:,1], c=colors)\n",
    "for i, label in enumerate(df_all['Header']):\n",
    "    plt.text(X_umap[i,0], X_umap[i,1], label, fontsize=8)\n",
    "plt.title(\"UMAP projection of Genome Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 15: Hierarchical Clustering Dendrogram\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "linked = linkage(X, 'ward')\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(linked, labels=df_all['Header'].values)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genome_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
