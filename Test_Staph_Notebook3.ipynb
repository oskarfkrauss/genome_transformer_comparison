{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeac3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0 install required packages TODO: rename this notebook\n",
    "%pip install transformers torch pandas numpy matplotlib networkx seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d551483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 import required packages\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0247ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: function to parse FASTA files\n",
    "def parse_fasta(file_path: str):\n",
    "    '''\n",
    "    Parse fasta file (.fna) file into list of strings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the fasta sequence file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    seq : List\n",
    "        The sequence parsed so that each element is a line from the fasta file\n",
    "    '''\n",
    "    with open(file_path) as f:\n",
    "        seq = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # do not parse the header line\n",
    "            if line.startswith('>'):\n",
    "                continue\n",
    "            else:\n",
    "                seq.append(line)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9425a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: parse sequences from data file, at the moment, NOTE: at the moment\n",
    "# we are only considering the first line of the sequence\n",
    "strain_1 = parse_fasta(os.path.join('data', 'GCA_006094915.1', 'GCA_006094915.1_ASM609491v1_genomic.fna'))[0]\n",
    "strain_2 = parse_fasta(os.path.join('data', 'GCA_026167765.1', 'GCA_026167765.1_ASM2616776v1_genomic.fna'))[0]\n",
    "strain_3 = parse_fasta(os.path.join('data', 'GCA_900607265.1', 'GCA_900607265.1_BPH2003_genomic.fna'))[0]\n",
    "strain_4 = parse_fasta(os.path.join('data', 'GCA_900620245.1', 'GCA_900620245.1_BPH2947_genomic.fna'))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff98b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: Define masked embedding function\n",
    "def get_masked_embedding(sequence: str):\n",
    "    '''\n",
    "    Create an embedding of a genome sequence\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : str\n",
    "        The sequence we parsed earlier, as a single string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The mean embedding?\n",
    "    '''\n",
    "    # this splits the sequence into input:\n",
    "    #  - IDs e.g. [2, 312, ... , 3671] which each correspond to a different 6-mer string in the\n",
    "    # tokenizer's vocabulary\n",
    "    #  - an attention mask e.g. [1, 1, 1, 0, 0] which tells the embedding model which IDs to\n",
    "    # process. In our case (for now), it'll be tensor of 1s since we are including every 6-mer\n",
    "    # string\n",
    "    tokens = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    # outputs is an encoding for each *token* for each layer of the (32-layer) Transformer,\n",
    "    # it has length 33 since layer 0 is an 'embedding layer'\n",
    "    # to get the 'final' embeddings, take the last layer NOTE: we may not always want the last\n",
    "    # layer\n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    attention_mask = attention_mask.unsqueeze(-1)\n",
    "    masked_embeddings = embeddings * attention_mask\n",
    "    mean_embedding = masked_embeddings.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return mean_embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_for_strain_1 = get_masked_embedding(strain_1)\n",
    "embedding_for_strain_2 = get_masked_embedding(strain_2)\n",
    "embedding_for_strain_3 = get_masked_embedding(strain_3)\n",
    "embedding_for_strain_4 = get_masked_embedding(strain_4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genome_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
